# RAG Fundamentals: How This System Actually Works\n\n## What RAG Really Means (Not the Marketing Version)\n\nRAG isn't magic - it's a practical pattern for making LLMs useful with your specific data. Here's what actually happens:\n\n**Without RAG**: LLM only knows what it learned during training (cutoff date, no your documents)\n**With RAG**: LLM gets relevant chunks from your documents as context before answering\n\nThat's it. The \"augmentation\" is just fancy prompt engineering with retrieved context.\n\n## The Three-Stage Pipeline\n\n### Stage 1: Retrieval\n**What happens**: Convert user question to embedding, find similar document chunks\n**Key insight**: This is pure similarity search - no understanding of whether chunks actually answer the question\n**Common failure**: High similarity but wrong content (\"car maintenance\" matches \"car insurance\" semantically)\n\n### Stage 2: Context Assembly  \n**What happens**: Take top chunks and format them into a prompt\n**Key decisions**: \n- How many chunks? (I use 3 - more context vs longer prompts)\n- What order? (Most relevant first due to primacy bias)\n- How to separate? (Clear delimiters help LLM parse context)\n\n### Stage 3: Augmented Generation\n**What happens**: LLM generates answer using both its training and provided context\n**Key insight**: LLM can still hallucinate even with good context\n**Success factors**: Clear instructions, appropriate temperature, fallback handling\n\n## Why Embeddings Work Better Than Keyword Search\n\nFrom building both approaches:\n\n**Keyword search problems I hit**:\n- Exact matching: \"How do I fix my car?\" misses \"automotive repair guide\"\n- Synonym blindness: \"automobile\" vs \"car\" vs \"vehicle\"\n- Context ignorance: \"bank\" could be financial or riverbank\n\n**Embedding advantages**:\n- Semantic understanding: Captures meaning, not just words\n- Robust to paraphrasing: Different ways of asking same question\n- Context awareness: Same word in different contexts embeds differently\n\n**Embedding limitations I've found**:\n- Miss exact matches: Model numbers, specific dates, proper names\n- Biased by training data: Better on common topics than specialized domains\n- No boolean logic: Can't do \"cars AND maintenance NOT insurance\"\n\n## The Chunking Problem\n\nThis is more important than most people realize:\n\n**Why chunk at all?**\n- Embedding models have context limits (512 tokens for many models)\n- Long documents embed to \"average\" of all topics\n- Retrieval precision: Want specific relevant sections, not entire documents\n\n**How chunk size affects everything**:\n- **Too small**: Fragments concepts, poor embeddings, missing context\n- **Too large**: Multiple topics per chunk, poor retrieval precision\n- **Just right**: Complete concepts with sufficient context\n\n**The overlap dilemma**:\n- No overlap: Key information split across boundaries\n- Too much overlap: Redundant content confuses LLM\n- Sweet spot: Enough to preserve context continuity\n\n## Similarity Scores: What They Actually Mean\n\nFrom testing thousands of queries:\n\n**Score ranges I've observed**:\n- **0.8-1.0**: Almost certainly relevant (rare, usually exact matches)\n- **0.6-0.8**: Very likely relevant, good semantic match\n- **0.4-0.6**: Possibly relevant, related concepts\n- **0.2-0.4**: Probably not relevant, weak connection\n- **<0.2**: Almost certainly irrelevant\n\n**Critical insight**: High similarity ≠ correct answer\n\nA chunk about \"car maintenance schedules\" might score 0.85 for \"when should I change my oil?\" but not contain the specific interval. The LLM still needs to extract the actual answer.\n\n## Why RAG Systems Fail (And How to Spot It)\n\n**Retrieval failures**:\n- Question outside knowledge base → Low similarity scores across all chunks\n- Information spread across multiple sections → No single chunk has complete answer\n- Embedding model bias → Technical jargon embeds poorly\n\n**Generation failures**:\n- Confident hallucination → LLM sounds certain but invents facts\n- Context confusion → Mixes information from different chunks incorrectly\n- Partial extraction → Uses only part of available context\n\n**System-level failures**:\n- PDF parsing errors → Garbled text creates meaningless chunks\n- Chunking boundary issues → Key information split across chunks\n- Prompt engineering problems → LLM doesn't follow instructions properly\n\n## The Evaluation Challenge\n\nUnlike traditional ML, RAG systems are hard to evaluate automatically:\n\n**Why standard metrics don't work**:\n- No ground truth labels for most documents\n- \"Correct\" answers often have multiple valid formulations\n- Context relevance is subjective\n- Answer quality depends on both retrieval and generation\n\n**What I actually do for evaluation**:\n1. **Manual spot checks**: Sample random queries, assess answer quality\n2. **Similarity score monitoring**: Track average scores over time\n3. **Failure case collection**: Keep examples of bad outputs\n4. **User feedback**: When available, track satisfaction\n\n**Red flags I watch for**:\n- Dropping average similarity scores (knowledge base drift)\n- Confident answers to out-of-scope questions (hallucination)\n- Repetitive or template-like responses (prompt issues)\n\nThis is messy and subjective, but it's the reality of RAG evaluation without labeled data.