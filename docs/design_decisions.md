# Design Decisions & Trade-offs\n\n## Embedding Model: all-MiniLM-L6-v2\n\n**Why I chose this over alternatives**:\n\n**vs OpenAI text-embedding-ada-002**:\n- ✅ Runs locally (no API calls, faster, more private)\n- ✅ No per-query costs\n- ✅ Consistent availability\n- ❌ Lower quality on specialized domains\n- ❌ Smaller context window (512 vs 8192 tokens)\n\n**vs larger sentence-transformers models**:\n- ✅ Fast inference (384 dims vs 768+ for larger models)\n- ✅ Lower memory usage\n- ✅ Good enough for general business documents\n- ❌ Less nuanced understanding of complex topics\n\n**Real-world impact**: For general document Q&A, the speed/quality trade-off works well. For highly technical or domain-specific content, I'd consider fine-tuning or using larger models.\n\n## Chunk Size: 500 Words with 50-Word Overlap\n\n**How I arrived at these numbers**:\n\nI tested on a corpus of business documents (reports, manuals, policies):\n\n**100-word chunks**:\n- Retrieval precision: Good (specific concepts)\n- Context completeness: Poor (fragmented ideas)\n- Embedding quality: Poor (insufficient context)\n- Result: Lots of \"I need more information\" responses\n\n**1000-word chunks**:\n- Retrieval precision: Poor (multiple topics per chunk)\n- Context completeness: Good (full concepts)\n- Embedding quality: Poor (averaged across topics)\n- Result: Relevant info buried in irrelevant content\n\n**500-word chunks**:\n- Retrieval precision: Good (usually 1-2 related concepts)\n- Context completeness: Good (complete thoughts)\n- Embedding quality: Good (sufficient context)\n- Result: Best balance for most document types\n\n**50-word overlap decision**:\nI kept hitting the \"boundary problem\" - key information split across chunks. Examples:\n- Chunk N ends: \"The solution requires three steps:\"\n- Chunk N+1 starts: \"1. First, configure the system...\"\n\n50 words ≈ 2-3 sentences, usually enough to preserve context flow.\n\n**Trade-offs I accepted**:\n- Some redundancy (same text in multiple chunks)\n- Doesn't work well for tables, code, or highly structured content\n- Fixed strategy regardless of document type\n\n## FAISS vs Alternatives\n\n**Why FAISS over database vector search**:\n\n**vs PostgreSQL pgvector**:\n- ✅ Simpler setup (no database configuration)\n- ✅ Faster for small-medium datasets\n- ✅ More vector index options\n- ❌ In-memory only (no persistence in my implementation)\n- ❌ No concurrent access handling\n\n**vs Pinecone/Weaviate**:\n- ✅ No external dependencies\n- ✅ No API costs\n- ✅ Full control over indexing\n- ❌ No built-in scaling\n- ❌ No metadata filtering\n\n**IndexFlatIP choice**:\nI use exact search (IndexFlatIP) over approximate (IVF, HNSW) because:\n- Dataset size < 10k chunks (exact search is fast enough)\n- No approximation errors\n- Simpler debugging (deterministic results)\n\nFor production scale, I'd switch to approximate methods.\n\n## OpenRouter vs Direct LLM APIs\n\n**Why OpenRouter over OpenAI directly**:\n- ✅ Often better pricing for experimentation\n- ✅ Easy model switching (GPT-3.5 → GPT-4 → Claude)\n- ✅ More generous rate limits\n- ✅ Unified API for multiple providers\n- ❌ Additional dependency/point of failure\n- ❌ Less control over model parameters\n\n**GPT-3.5-turbo choice**:\n- ✅ Available on free tier\n- ✅ Fast inference\n- ✅ Good instruction following\n- ❌ Less capable than GPT-4 for complex reasoning\n- ❌ More prone to hallucination\n\nFor production, I'd use GPT-4 or Claude for better quality.\n\n## Prompt Engineering Approach\n\n**Simple vs Complex Prompting**:\n\nI deliberately chose simple prompts over advanced techniques:\n\n**Didn't use**:\n- Few-shot examples (would make prompts longer/more expensive)\n- Chain-of-thought reasoning (adds complexity, not always better)\n- Role-playing (\"You are an expert...\" - often unnecessary)\n- Multi-step reasoning (would need multiple API calls)\n\n**What I do use**:\n- Clear context separation\n- Explicit instructions to use provided context\n- Fallback instruction for insufficient information\n- Appropriate temperature setting\n\n**Reasoning**: For document Q&A, simple prompts work well and are easier to debug. Complex prompting techniques add value for reasoning tasks but not basic information extraction.\n\n## SQLite for Logging\n\n**Why SQLite over alternatives**:\n\n**vs PostgreSQL**:\n- ✅ Zero configuration\n- ✅ Single file database\n- ✅ Perfect for logging/analytics workload\n- ❌ No concurrent writes (fine for single-user demo)\n- ❌ Limited scalability\n\n**vs NoSQL (MongoDB, etc.)**:\n- ✅ Structured query capabilities\n- ✅ ACID transactions\n- ✅ Simpler for relational data (queries → responses)\n- ❌ Less flexible schema\n\n**What I track**:\n- Query text and responses (for debugging)\n- Similarity scores (for quality monitoring)\n- Timestamps (for usage patterns)\n- Document metadata (for provenance)\n\n## Architecture Decisions\n\n**Synchronous vs Asynchronous**:\nI use async/await throughout because:\n- LLM API calls are I/O bound (perfect for async)\n- FastAPI handles async naturally\n- Better resource utilization\n- Easier to add concurrent processing later\n\n**Global instances vs Dependency Injection**:\nI use global instances for:\n- Embedding model (expensive to load repeatedly)\n- FAISS index (shared state)\n- LLM client (stateless, can be shared)\n\nTrade-off: Less testable, but simpler for demo purposes.\n\n**Error Handling Philosophy**:\nI return error messages as strings rather than raising exceptions for LLM failures because:\n- Users need to see what went wrong\n- Partial failures are common (PDF parsing, API timeouts)\n- Better UX than generic error pages\n\n## What I Would Change for Production\n\n**Immediate needs**:\n1. **Persistent FAISS index** (save/load from disk)\n2. **Batch processing** (handle multiple PDFs efficiently)\n3. **Better error handling** (structured errors, retry logic)\n4. **Configuration management** (environment-specific settings)\n\n**Scaling needs**:\n1. **Vector database** (Pinecone, Weaviate, or pgvector)\n2. **Async document processing** (background jobs)\n3. **Caching layer** (Redis for frequent queries)\n4. **Monitoring** (metrics, alerting, performance tracking)\n\n**Quality improvements**:\n1. **Hybrid search** (combine vector + keyword)\n2. **Re-ranking** (cross-encoder for better ordering)\n3. **Evaluation framework** (automated quality assessment)\n4. **A/B testing** (compare different approaches)\n\nThese changes would significantly improve the system but add substantial complexity. For demonstrating RAG fundamentals, the current approach strikes the right balance.